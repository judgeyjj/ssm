import torch
import torch.nn as nn
import numpy as np
from config import get_default_config
from generator import build_generator

def set_seed(seed=42):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    np.random.seed(seed)

def check_causality_and_streaming():
    print("\n" + "="*50)
    print("ğŸ§ª å¼€å§‹å…¨é¢ Sanity Check")
    print("="*50)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    # 1. åˆå§‹åŒ–
    config = get_default_config()
    # ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œå‡å°æ¨¡å‹è§„æ¨¡
    config.model.hidden_channels = 32
    config.model.num_moe_layers = 2
    
    model = build_generator(config).to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ | å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

    # 2. æ¨¡æ‹Ÿæ•°æ®
    # 1ç§’ 16kHz éŸ³é¢‘ -> 16000 é‡‡æ ·ç‚¹
    # Batch size = 2
    B, C, L = 2, 1, 16000
    x = torch.randn(B, C, L).to(device)
    
    # 3. æµ‹è¯• Forward (å¹¶è¡Œæ¨¡å¼)
    print("\n[Step 1] æµ‹è¯•æ ‡å‡† Forward Pass (Parallel Mode)...")
    with torch.no_grad():
        y_parallel, aux_loss = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {y_parallel.shape}")
    print(f"Aux Loss: {aux_loss.item():.4f}")
    
    # æ£€æŸ¥ä¸Šé‡‡æ ·å€ç‡
    target_len = L * 3
    if y_parallel.shape[-1] == target_len:
        print(f"âœ… è¾“å‡ºé•¿åº¦æ­£ç¡® ({target_len})")
    else:
        print(f"âŒ è¾“å‡ºé•¿åº¦é”™è¯¯! æœŸæœ› {target_len}, å®é™… {y_parallel.shape[-1]}")
        return

    # æ£€æŸ¥è¾“å‡ºèŒƒå›´ (Refiner æœ‰ Tanh)
    min_val, max_val = y_parallel.min().item(), y_parallel.max().item()
    if min_val >= -1.01 and max_val <= 1.01: # ç•™ä¸€ç‚¹æµ®ç‚¹è¯¯å·®ä½™é‡
        print("âœ… è¾“å‡ºèŒƒå›´æ­£ç¡® [-1, 1]")
    else:
        print(f"âŒ è¾“å‡ºèŒƒå›´å¼‚å¸¸: [{min_val:.4f}, {max_val:.4f}]")

    # 4. æµ‹è¯•æµå¼æ¨ç† (Streaming Mode)
    print("\n[Step 2] æµ‹è¯•æµå¼æ¨ç† (Chunk-by-Chunk)...")
    
    # æ¨¡æ‹Ÿæµå¼ï¼šåˆ‡åˆ†æˆå°å— (ä¾‹å¦‚ 20ms = 320 ç‚¹)
    # chunk_size = 320 # çœŸå®åœºæ™¯
    chunk_size = 1600 # ç¨å¾®å¤§ç‚¹æ–¹ä¾¿æµ‹è¯•ï¼Œå¿…é¡»èƒ½è¢« L æ•´é™¤
    total_chunks = L // chunk_size
    
    buffer_dict = None # åˆå§‹åŒ– buffer
    output_chunks = []
    
    try:
        with torch.no_grad():
            for i in range(total_chunks):
                chunk = x[:, :, i*chunk_size : (i+1)*chunk_size]
                
                # è°ƒç”¨æµå¼æ¥å£
                out_chunk, buffer_dict = model.infer_stream(chunk, buffer_dict)
                output_chunks.append(out_chunk)
                
            y_stream = torch.cat(output_chunks, dim=-1)
            
        print(f"æµå¼è¾“å‡ºå½¢çŠ¶: {y_stream.shape}")
        
        # 5. éªŒè¯ä¸€è‡´æ€§ (Causality Check)
        # æµå¼å¤„ç†çš„ç»“æœåº”è¯¥ä¸ä¸€æ¬¡æ€§ Forward çš„ç»“æœï¼ˆå‡ ä¹ï¼‰å®Œå…¨ä¸€è‡´
        
        # å¯¹é½é•¿åº¦ï¼ˆå¦‚æœæœ‰ padding å·®å¼‚ï¼‰
        min_len = min(y_parallel.shape[-1], y_stream.shape[-1])
        
        # åªæ¯”è¾ƒä¸­é—´éƒ¨åˆ†ï¼Œé¿å¼€åˆå§‹çŠ¶æ€å¸¦æ¥çš„å·®å¼‚ï¼ˆInitial transientï¼‰
        # å¯¹äºå› æœå·ç§¯ï¼Œé€šå¸¸å‰å‡ ä¸ªç‚¹ä¼šæœ‰å·®å¼‚ï¼Œå› ä¸º Forward æ¨¡å¼é»˜è®¤ pad 0ï¼Œ
        # è€Œ Streaming æ¨¡å¼ç¬¬ä¸€å—ä¹Ÿ pad 0ï¼Œç†è®ºä¸Šåº”è¯¥ä¸€è‡´ã€‚
        diff = torch.abs(y_parallel[..., :min_len] - y_stream[..., :min_len])
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print(f"æœ€å¤§è¯¯å·®: {max_diff:.2e}")
        print(f"å¹³å‡è¯¯å·®: {mean_diff:.2e}")
        
        if mean_diff < 1e-4: # å®½æ¾ä¸€ç‚¹
            print("âœ… Streaming ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ (æ¨¡å‹æ˜¯ä¸¥æ ¼å› æœçš„)")
        else:
            print("âš ï¸ Streaming ä¸€è‡´æ€§æ£€æŸ¥è­¦å‘Š: è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨éå› æœæ“ä½œæˆ–çŠ¶æ€ç®¡ç† Bug")
            
    except Exception as e:
        print(f"âŒ æµå¼æ¨ç†å´©æºƒ: {e}")
        import traceback
        traceback.print_exc()

    # 6. æ¢¯åº¦åå‘ä¼ æ’­æ£€æŸ¥
    print("\n[Step 3] æ¢¯åº¦æ£€æŸ¥...")
    model.train()
    # éœ€è¦é‡æ–° forward å› ä¸ºä¹‹å‰çš„æ˜¯ no_grad
    x.requires_grad = True
    y_train, aux_loss = model(x)
    loss = y_train.mean() + aux_loss
    
    try:
        loss.backward()
        print("âœ… åå‘ä¼ æ’­æˆåŠŸ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°æ²¡æœ‰æ¢¯åº¦ (Dead parameters)
        # æ³¨æ„ï¼šSwitch Transformer çš„ aux loss æœ‰æ—¶ä¼šå¯¼è‡´æœªé€‰ä¸­çš„ expert æ— æ¢¯åº¦ï¼Œè¿™åœ¨å•æ¬¡è¿­ä»£ä¸­æ˜¯æ­£å¸¸çš„
        no_grad_params = [name for name, p in model.named_parameters() if p.grad is None]
        
        if len(no_grad_params) > 0:
            print(f"â„¹ï¸  æœ¬æ¬¡è¿­ä»£æœ‰ {len(no_grad_params)} ä¸ªå‚æ•°æ²¡æœ‰æ¢¯åº¦ (MoE ä¸­æœªè¢«é€‰ä¸­çš„ Expert æ— æ¢¯åº¦å±æ­£å¸¸ç°è±¡):")
            # for name in no_grad_params[:5]:
            #     print(f"  - {name}")
        else:
            print("âœ… æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦")
            
    except Exception as e:
        print(f"âŒ åå‘ä¼ æ’­å¤±è´¥: {e}")

if __name__ == "__main__":
    set_seed()
    check_causality_and_streaming()

