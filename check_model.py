"""
FASS-MoE Model Sanity Check Script.

éªŒè¯æ¨¡å‹çš„:
1. Forward pass è¾“å‡ºå½¢çŠ¶å’ŒèŒƒå›´
2. Streaming å’Œ Forward çš„ä¸€è‡´æ€§ (ä½¿ç”¨ RMSNorm + WeightNorm)
3. æ¢¯åº¦ä¼ æ’­
"""

import torch
import numpy as np


def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


def main():
    print("\n" + "=" * 60)
    print("ğŸ§ª FASS-MoE Model Sanity Check")
    print("=" * 60)
    
    # Import inside function to avoid issues during error reporting
    from config import get_default_config
    from generator import build_generator
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    # Create smaller model for testing
    config = get_default_config()
    config.model.hidden_channels = 32
    config.model.num_moe_layers = 2
    config.model.num_experts = 4
    
    set_seed(42)
    model = build_generator(config).to(device)
    model.eval()
    
    param_count = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"âœ… Model loaded | Parameters: {param_count:.2f}M")
    
    # Test input
    B, C, L = 2, 1, 16000  # 1 second at 16kHz
    x = torch.randn(B, C, L, device=device)
    
    # =========================================
    # [Step 1] Forward Pass
    # =========================================
    print("\n" + "-" * 40)
    print("[Step 1] Testing Forward Pass (Parallel Mode)")
    print("-" * 40)
    
    set_seed(42)
    with torch.no_grad():
        y_forward, aux_loss = model(x)
    
    print(f"Input shape:  {x.shape}")
    print(f"Output shape: {y_forward.shape}")
    print(f"Aux Loss:     {aux_loss.item():.4f}")
    
    expected_len = L * 3  # 3x upsampling
    if y_forward.shape[-1] == expected_len:
        print(f"âœ… Output length correct ({expected_len})")
    else:
        print(f"âŒ Output length wrong! Expected {expected_len}, got {y_forward.shape[-1]}")
        return
    
    out_min, out_max = y_forward.min().item(), y_forward.max().item()
    if -1.0 <= out_min and out_max <= 1.0:
        print(f"âœ… Output range correct: [{out_min:.4f}, {out_max:.4f}]")
    else:
        print(f"âš ï¸  Output range warning: [{out_min:.4f}, {out_max:.4f}]")
        print("   (Expected [-1, 1] due to tanh)")
    
    # =========================================
    # [Step 2] Streaming Consistency
    # =========================================
    print("\n" + "-" * 40)
    print("[Step 2] Testing Streaming Consistency")
    print("-" * 40)
    
    chunk_size = 1600  # 100ms chunks at 16kHz
    num_chunks = L // chunk_size
    
    print(f"Processing {num_chunks} chunks of {chunk_size} samples each")
    
    set_seed(42)
    state = None
    output_chunks = []
    
    try:
        with torch.no_grad():
            for i in range(num_chunks):
                chunk = x[:, :, i * chunk_size : (i + 1) * chunk_size]
                out_chunk, state = model.infer_stream(chunk, state)
                output_chunks.append(out_chunk)
            
            y_stream = torch.cat(output_chunks, dim=-1)
        
        print(f"Stream output shape: {y_stream.shape}")
        
        # Compare
        min_len = min(y_forward.shape[-1], y_stream.shape[-1])
        diff = torch.abs(y_forward[..., :min_len] - y_stream[..., :min_len])
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print(f"Max difference:  {max_diff:.2e}")
        print(f"Mean difference: {mean_diff:.2e}")
        
        # Strict threshold for streaming consistency
        if mean_diff < 1e-5:
            print("âœ… Streaming PERFECTLY consistent with forward()")
        elif mean_diff < 1e-3:
            print("âœ… Streaming consistent (minor numerical differences)")
        elif mean_diff < 1e-2:
            print("âš ï¸  Streaming mostly consistent, small differences")
        else:
            print("âŒ Streaming INCONSISTENT with forward()")
            print("   This may indicate:")
            print("   - LayerNorm/BatchNorm depending on sequence length")
            print("   - State management bugs in conv/mamba buffers")
            print("   - Non-causal operations in the forward path")
            
    except Exception as e:
        print(f"âŒ Streaming failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # =========================================
    # [Step 3] Gradient Check
    # =========================================
    print("\n" + "-" * 40)
    print("[Step 3] Testing Gradient Propagation")
    print("-" * 40)
    
    model.train()
    y_train, aux_loss = model(x)
    loss = y_train.mean() + aux_loss
    
    try:
        loss.backward()
        print("âœ… Backward pass successful")
        
        no_grad_params = [name for name, p in model.named_parameters() if p.grad is None]
        if len(no_grad_params) == 0:
            print("âœ… All parameters have gradients")
        else:
            print(f"âš ï¸  {len(no_grad_params)} parameters without gradients:")
            for name in no_grad_params[:5]:
                print(f"   - {name}")
            if len(no_grad_params) > 5:
                print(f"   ... and {len(no_grad_params) - 5} more")
                
    except Exception as e:
        print(f"âŒ Backward failed: {e}")
        return
    
    # =========================================
    # [Step 4] Causality Check
    # =========================================
    print("\n" + "-" * 40)
    print("[Step 4] Causality Check")
    print("-" * 40)
    
    model.eval()
    set_seed(42)
    
    # Create two inputs that differ only in the second half
    x1 = torch.randn(1, 1, 8000, device=device)
    x2 = x1.clone()
    x2[:, :, 4000:] = torch.randn(1, 1, 4000, device=device)  # Different second half
    
    with torch.no_grad():
        y1, _ = model(x1)
        y2, _ = model(x2)
    
    # First half outputs should be identical (causal model)
    first_half_diff = torch.abs(y1[:, :, :12000] - y2[:, :, :12000]).max().item()
    
    if first_half_diff < 1e-5:
        print("âœ… Model is CAUSAL (first half outputs identical)")
    else:
        print(f"âŒ Model may not be causal! First half diff: {first_half_diff:.2e}")
        print("   Future inputs are affecting past outputs")
    
    # =========================================
    # Summary
    # =========================================
    print("\n" + "=" * 60)
    print("ğŸ“Š Summary")
    print("=" * 60)
    print(f"Model Parameters: {param_count:.2f}M")
    print(f"Streaming Error:  {mean_diff:.2e}")
    print(f"Output Range:     [{out_min:.4f}, {out_max:.4f}]")
    print(f"Causality:        {'âœ… Pass' if first_half_diff < 1e-5 else 'âŒ Fail'}")
    print("=" * 60)


if __name__ == "__main__":
    main()
